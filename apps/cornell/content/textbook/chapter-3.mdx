---
title: "Learning Analytics Dashboard"
page_slug: learning-an-1
summary: true
quiz: false
---
<div className="content-chunk" data-subsection-id = "Abstract-94pt">
## Abstract
_Joris Klerkx, Katrien Verbert, and Erik Duval_  
_Department of Computer Science, KU Leuven, Belgium_

          This chapter presents learning analytics dashboards that visualize learning traces to give users insight into the learning process. Examples are shown of what data can be visualized. In addition, guidelines on how to get started with the development of learning analytics dashboards are presented for practitioners and researchers. 

<Warning title="Keywords">  
\- Information visualization  
\- learning analytics dashboards  
</Warning>

          In recent years, many learning analytics dashboards have been deployed to support insight into learning data. The objectives of these dashboards include providing feedback on learning activities, supporting reflection and decision making, increasing engagement and motivation, and reducing dropout. These learning analytics dashboards apply information visualization techniques to help teachers, learners, and other stakeholders explore and understand relevant user traces collected in various (online) environments. The overall objective is to improve (human) learning. 

          The goal of this chapter is to provide a guide to practitioners and researchers who want to get started with the development and evaluation of learning analytics dashboards. We provide guidance, and several examples, to address the following items:

1.  What kind of data can be visualized?
2.  For whom are the visualizations intended (learner, teacher, manager, researcher, other)?
3.  Why: what is the goal of the visualization?
4.  How can the data be visualized? Which interaction techniques can be applied? What tools, libraries, data formats, et cetera can be used for the technical implementations? What workflow and recipe can be used to develop the visualization?

          In addition to these four questions, we elaborate on evaluation aspects that assess the usefulness and potential impact of the approach.   
more work on this paper.
</div>
<div className="content-chunk" data-subsection-id = "Background-472t">
## Background
### **To Augment the Human Intellect**

There is a strong contrast between intelligent systems that try to make decisions on behalf of people, such as intelligent tutoring systems (Brusilovsky, 2000) and educational data mining systems (Santos et al., 2015), and systems that try to empower people to make better-informed decisions. For instance, visual analytics systems (Shneiderman & Bederson, 2003) provide a clear overview of the context, the decisions that can be made, and the potential implications of those decisions. 

          Data mining plays to the strength of computers to do number crunching, while visualization techniques play to the remarkable perceptual abilities that humans possess. The difference between the two approaches is like the difference between a self-driving car and a car with a human driver. Data mining uses automatic pattern matching for remote control while the dashboard provides visual communication to assist a human driver who remains in control of the vehicle.

          There is a certain philosophical or ethical side to this notion of two approaches as well: if learners are always told what to do next, how can they develop the typical 21st-century skills of collaboration, communication, critical thinking, and creativity? Or, at a more fundamental level, how can they become citizens equipped with the knowledge, skills, and attitudes to participate fully in society? In this chapter, we focus on methods that augment the human intellect, through visual approaches for learning analytics (Engelbart, 1995).
</div>
<div className="content-chunk" data-subsection-id = "Information-Visualization-529t">
## Information Visualization
Information visualizations is the use of interactive visual representations to amplify cognition (Card, Mackinlay, & Shneiderman, 1999). It typically focuses on abstract data without a straightforward representation in 2-D or 3-D space. Visual analytics puts specific emphasis on building models and visualizing these in order to better understand or refine the models. A very useful goal of information visualization is to rely on human perceptual abilities for pattern discovery (trends, gaps, outliers, clusters). These patterns often become more apparent visually than numerically. As Ware (2004) explains it:

          The human visual system is a pattern seeker of enormous power and subtlety. The eye and the visual cortex of the brain form a massively parallel processor that provides the highest-bandwidth channel into human cognitive centers. At higher levels of processing, perception and cognition are closely interrelated, which is the reason why the words "understanding" and "seeing" are synonymous. (p. xxvi)

          As such, visualization ha the potential to be more precise and revealing than conventional statistical computations (Tufte, 2001).

          Static visualizations (i.e., an image) typically provide answers to a limited number of questions that a user might have about a data set. For example, so-called infographics are often used for storytelling in journalism. However, looking at an evocative visualization often leads to new questions that can only be answered by interacting with the data itself (Few, 2009). Adding dynamic interaction techniques to the visualization, therefore, is often necessary to design meaningful visualization tools that encourage exploratory data analysis.

          Another advantage of visualization is the ability to reveal problems with the data itself; for instance, about the way the data has been collected. Especially in the case of learning analytics, where (semi-) automated trackers often capture traces of learner activities, this advantage is valuable for quality control.
</div>
<div className="content-chunk" data-subsection-id = "What-For,-Whom,-Why,-How?-473t">
## What For, Whom, Why, How?
What follows is a non-exhaustive overview; it is important to recognize the variety of approaches. This variety is not surprising given the wide variety of learning analytics data that can be visualized, for a wide variety of audiences and reasons, in a wide variety of ways.

          Verbert et al. (2014) present a survey of learning analytics dashboard applications " ranging from a small applications to learnscapes on large public displays" (p. 1499). Dashboards, they say, "typically capture and visualize traces of learning activities, in order to promote awareness, reflection, and sense-making, and to enable learners to define goals and track progress towards these goals" (p. 1499). The paper makes useful distinction between various types of dashboards:

1.  Dashboards that support traditional face-to-face lectures, so as to enable the teacher to adapt the teaching, or to engage students during lecture sessions.
2.  Dashboards that support face-to-face group work and classroom orchestration, for instance by visualization activities of both individual learners and groups of learners.
3.  Dashboards that support online or blended learning: an early famous example is Course Signals that visualizes predicted learning outcomes as a traffic light, based on grades in the course so far, time on task and past performance (Arnold & Pitstilli, 2012). More sophisticated and complex visualizations for detailed analysis of course activity by teachers are the focus of the Student Activity Meter (Goaverts, Verbert, Duval, & Pardo, 2012). SNAPP focuses on the visualization of social activity of learners (Bakharia & Dawson, 2011). 

          In terms of what is being tracked, the possibilities continue to expand, as new online trackers become available, capturing more detail of what learners and teachers do. As well, new sensors proliferate that can likewise capture what people do in the analog world. This second data source is evolving especially rapidly, with mobile devices that now include sensors to report physiological, emotional, and other kinds of learner characteristics that have so far mostly eluded automated capturing. Besides tracking, self-reporting can also be a valuable source of data. Although more error-pine and difficult to sustain systematically, self-reporting offers an opportunity for awareness, reflection, and self-analysis.
</div>
<div className="content-chunk" data-subsection-id = "What-For,-Whom,-Why,-How?-Chunk-2-531t">
          As for what can be incorporated into a dashboard, Verbert et al. (2014) lists the following kinds of data:

1.  _Artefacts produced_ by learners, including blog posts, shared documents, software, and other artefacts that would often end up in a student project portfolio.
2.  _Social interaction_, including speech in face-to-face group work, blog comments, Twitter or discussion forum interactions.
3.  _Resource use_ can include consultation of documents (manuals, web pages, slides), views of videos, et cetera. Techniques like software trackers and eye-tracking can provide detailed information about what parts of resources exactly are being used and how.
4.  _Time spent_ can be useful for teachers to identify students at risk and for students to compare their own efforts with those of their peers.
5.  _Test and self-assessment_ results can provide an indication of learning progress.

<Image
  style="aspect-ratio:1390/939;"
  src="https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/pic1.png-c8eb390be3a3f1de54f47aa2ce2d9712.png"
  alt="Figure 12.1"
  width="1390"
  height="939">
Figure 12.1. (Top) Navi Badgeboard - Personal Badge Overview: A student's badge overview for a given period; (bottom) Navi Surface: students actively using the tabletop display application during a face-to-face session (Charleer et al., 2013).
</Image>

          Figure 12.1 presents one of our more recent dashboards (Charleer, Klerkx, Odriozola, Luis, & Duval, 2013). The dashboard tracks social data from blogs and Twitter. Such data, categorized as _artefacts produced_, is then visualized for _students_. The goal is to support awareness about learning progress and to enable discussion in class. To support such awareness and discussion, social interactions of students are abstracted in the form of learning badges for students to earn. Students can then explore which badges they have earned (Figure 12.2, top) through the visualization of _icons_ and _colour cues._ Gray badges have not yet been earned. The bottom part of Figure 12.2 shows a visualization,  developed for collaborative use on a tabletop that uses a node _link diagram_ to enable further exploration of these badges. Among other things, students have earned specific badges as a means to compare and discuss learning progress.
</div>
<div className="content-chunk" data-subsection-id = "What-For,-Whom,-Why,-How?-Chunk-3-530t">
<Image
  style="aspect-ratio:1227/561;"
  src="https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/fig12.2.png-fb4ffb4fbe26a1f84f9800a93bcd7209.png"
  alt="Figure 12.2"
  width="1227"
  height="561">
Figure 12.2. Muva dashboard that represents the likelihood of failing a specific course (Ochoa et al., 2016).
</Image>

          Figure 12.2 shows a dashboard that uses grades to predict a student’s chances of failing a particular course (Ochoa, Verbert, Chiluiza, & Duval, 2016) before she starts. The dashboard is intended to support teachers in giving advice to students on their learning trajectories. More specifically, the dashboard presents the likelihood (68%) of this particular student failing a course in which she is interested. The dashboard uses colour cues to indicate whether the risk of failure, based on past performance, is low (green), medium (yellow), or high (red). Depending on the outcome, the teacher can advise the student to take the course or to discuss alternatives, such as first taking a prerequisite course. The dashboard also supports several interaction techniques that enable the teacher to indicate which data should be taken into account to generate this prediction, including sliders at the bottom that enable the teacher to specify the range of data in terms of years. For example, if a student did poorly in Biology in Grade 10 btu worked harder and did well in Grade 12, the Grade 10 mark can be disregarded.
</div>
<div className="content-chunk" data-subsection-id = "How-to-Get-Started-474t">
## How to Get Started
To leverage the advanced perceptual abilities of humans to help them explore and discover patterns, a designer must create a visual representation or encoding of the data (Card et al., 1999). Several steps, outlined below, can be distinguished in this design process.

### **Understand Your Goals**

The first step is getting to know the problem domain, the data set, the intended end-users of the tool, the typical tasks they should be able to perform, and so on. The following questions need to be answered at this stage:

1.  _Why_: What is the goal of the visualization? What questions about the data should it answer?
2.  _For whom_: For whom is the visualization intended? Are the people involved specialists in the domain or in visualization?
3.  _What_: What data will the visualization display? Do these data exhibit a specific internal structure, like time, a hierarchy, or a network?
4.  _How:_ How will the visualization support the goal? How will people be able to interact with the visualization? What is the intended output device?

          By carefully examining and understanding the data set, a variety of questions about the data can be formed. Having these questions in mind can be useful when acquiring and filtering data for the dashboard. For example, consider a data set that contains the following learner traces:

* access to learning resources
* time on page in digital textbooks
* contributions to discussion fora
* time spent on assignments
</div>
<div className="content-chunk" data-subsection-id = "How-to-Get-Started-Chunk-2-532t">
          From these traces, we can define several relevant questions as a starting point in the design process. A teacher might ask questions like these:

* When did students start looking at the course material?
* What is the average time that a student spends reading the textbook?
* How many hours did Peter work on his assignment?
* How often did Peter ask a question on the discussion forum?

          A student will probably ask similar questions:

* How much time do I spend on an assignment, compared to other students?
* How much do I contribute to the discussion forum, compared to other students?

          In both cases, we deliberately only list questions that start with "what," "when," "how much," and "how often."  These specific, direct questions can be directly mapped in a data set. Questions like, "Why did this student have to enroll twice in this course?" the answer is more exploratory in nature. Indicators may be that he did not spend enough time on the course material, did not interact with fellow students on the discussion forum, started to study the course material too late, and so on. Another difficult question to answer would be, "Are students more eager to work on assignment 1 or assignment 2?" Even if much data is capture, it is difficult to answer questions involving human motivations based on a plurality of (un)known variables. Especially in the early phase of design, it is therefore often advisable and easier to focus on direct, specific questions.
</div>
<div className="content-chunk" data-subsection-id = "Acquire-and-Pre-Process-Your-Data-533t">
## Acquire and (Pre-)Process Your Data
Building a visual dashboard typically entails a data-gathering and preprocessing step. Visualization experts suggest that this step takes 80% of the time and effort versus all other steps. McDonnel and Elmqvist (2009) identify the following intermediary steps:

1.  **Acquiring raw data:** It is important to have a clear idea of where the data will come from (e.g., the log files for LMS, assessment results, other), and when the data will be updated (continuously, not at all, at specific intervals ). Will the data be available through an Application Programming Interface (API), an export file, or some other source?
2.  **Analyzing raw data:** Data me need to be cleaned if some values are missing or erroneous, or pre-processed to compute aggregate values (mean, minimum, maximum, et cetera). In data analysis, distribution can also be an issue: are there apparent outliers, clusters, et cetera?
3.  **Preparing and filtering data:** Using the initial questions from step 1, choose the relevant data from the pool of analyzed raw data.
</div>
<div className="content-chunk" data-subsection-id = "Mapping-Design-534t">
## Mapping Design
Important in the visual mapping design is to choose a representation that best answers the questions you want users to be able to answer, i.e., that serve your There exists a multitude of alternatives. One way to start is to look at the measurement or scale of each data characteristic. Nominal or qualitative scales differentiate objects based on discrete input domains, such as categories or other qualitative classifications to which they belong. Quantitative scales have continuous input domains (e.g., \[0. 100\]). Ordinal scales have discrete input domains where the order of the elements matters but the exact difference between the values does not. Depending on the scale of the data characteristic, one can choose how to encode this data visually. Figure 12.3 depicts Mackinlay's (1986) ranking of visual properties to encode quantitative, ordered, and categorical scales. For instance, the spatial position of an element is useful for encoding quantitative, ordered, and categorical differences. This is why scatterplots have been used so often to convey a variety of information. Length, on the other hand, can encode quantitative differences, but is of less value for encoding ordered and categorical differences. Shape is at the bottom of the ranking for visualizing quantitative and ordered differences, but is more often used to depict categorical data.

<Image
  style="aspect-ratio:958/613;"
  src="https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/fig12.3.png-d54d3c86d2fabb0eeaa9ed3d5e1c77a1.png"
  alt="fig12.3.png"
  width="958"
  height="613">
Figure 12.3. Mackinlay's (1986) ranking of visual properties for data characteristics on quantitative, ordered, and categorical scales. 
</Image>

          Low-fidelity prototypes such as paper sketches are often helpful during the design-mapping step. Figure 12.4 depicts an exercise given to the participants of the "Bring Your Own Data: Visual Learning Analytics" tutorial organized at the Learning Analytics Summer Institute (LASI) 2014. Participants included researches with good knowledge in learning analytics, but limited knowledge about visualization. They were asked to take 15 minutes to sketch all possible ways to visualize a simple data set of two numbers {75, 37}. The exercise illustrated to participants that from the moment they start sketching, it is not difficult to brainstorm visual encodings of data. This is reflected in the number of sketches that two teams of two persons each were able to generate in 15 minutes (see Figure 12.4a and 12.4b).
</div>
<div className="content-chunk" data-subsection-id = "How-to-Get-Started-Chunk-5-535t">
          By sketching, more ideas and questions about the data set are often raised, which in turn leads to new ideas for visualization. For example:

* Figure 12.4c: participants represented the difference between the numbers quite originally by relating them to age, where a person of 37 can easily lift weights, while a person of 73 might already need a walking stick.
* Figure 12.4d: adds muscle size.
* Figure 12.4e: uses shading of an equally sized circle with 75 versus 37 stripes
* Figure 12.4f: uses a position in a Cartesian coordinate system.
* Figure 12.4g: visualizes a part-to-whole relationship between the numbers.
* Figure 12.4h: assumes a time-based relationship between both numbers, which leads to a negative trend line.
* Figure 12.4i: uses point clouds.
* Figure 12.4j: visualizes an unbalanced scale to represent a difference in weight.
* Figure 12.4k: correlates the size of the figure with the size of the number. 

          After selecting a visual encoding, high-fidelity prototypes can be built using visualization tools (like Tableau, or even Microsoft Excel) or existing visualization libraries (like Processing or D3.js).

<Image
  style="aspect-ratio:937/700;"
  src="https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/fig12.4.png-c9f30674c73050c62dcd2d21c1789c66.png"
  alt="fig12.4.png"
  width="937"
  height="700">
Figure 12.4. Sketches of a small data set of two numbers {75, 37}.
</Image>

          Clearly some alternatives work better than others, depending on the contextualization (e.g., weight and age) and the ability to be interpreted by users (e.g., the mental model of a balanced scale). There is, therefore, no best way to visualize a data set, but some techniques have been proven to work better than others, for example:

* Pie charts are usually a bad idea (Few, 2009).
* Bar charts can be quite powerful.
* Coordinated graphs enable rich exploration. 
* 3-D graphics often do not convey any additional information and force the reader to deal with redundant and extraneous cues (Levy, Zacks, Tversky, & Schiano, 1996). 
* Scatterplots and parallel coordinates are good representations for depicting correlations. In addition, Harrison, Yang, Franconeri, & Chang (2014) found that among the stacked chart variants, the stacked bar significantly outperformed both the stacked area and stacked line. Elliot (2016) has presented a nice overview of these studies.
</div>
<div className="content-chunk" data-subsection-id = "How-to-Get-Started-Chunk-6-536t">
### **Documentation**

As with any design exercise, it is important to be explicit about:

1.  **Rationale**: Why were certain decisions made, what was the intent?
2.  **Alternatives**: Which alternatives were considered and why were they not withheld?
3.  **Evolution**: How has the design evolved from early sketches to a full-blown implementation? What was modified for conceptual reasons and what for implementation or other reasons (logistics, lack of time, other reasons)?
</div>
<div className="content-chunk" data-subsection-id = "How-to-Get-Started-Chunk-7-538t">
### **Add Interaction Techniques**

Visual analysis typically progresses in an iterative process of view of creation, exploration, and refinement (Heer & Schneiderman, 2012). Before analyzing which interaction techniques are useful for a specific visualization application, it is useful to understand the typical analytical tasks performed by teachers who want to understand how their students are doing in class. Several task taxonomies have been described in literature for this purpose. Common tasks include:

* Comparing values and patterns to find similarities and differences.
* Sorting items based on a variety of data values or metrics.
* Filtering values that satisfy a set of conditions.
* Highlighting data to make specific values stand out visually without making all other data disappear, as is the case with filtering data.
* Clustering or grouping similar items together; for example, by aggregating quantitative data (e.g., average, count, et cetera) to view it in a higher or lower level of detail.
* Annotating findings and thoughts.
* Bookmarking or recording a specific view on the data to enable effective navigation.

          Heer and Shneiderman (2012) is essential reading on interactive dynamics for visual analytics. The authors present a taxonomy of interactive dynamics that contribute to successful visual analytic tools. For each task category, various existing visualization systems are described with useful interaction techniques that support the task at hand, such as brushing and linking, histogram sliders, zoomable maps, dynamic query filter widgets, small multiple displays or trellis plots, multiple coordinated views, visual analysis histories, and so on.
</div>
<div className="content-chunk" data-subsection-id = "Evaluate-Continuously-537t">
## Evaluate Continuously
During the design process, the elaboration of concrete personas and scenarios can be very rewarding as it helps to focus the design, development, and evaluation of the visualization on what is relevant. It is very easy to get carried away with too much "eye candy" and lose track of the what, for whom, and why the visualization is being designed. Generally, a user-centred design (UCD) approach proceeds with iterative development that keeps the target users in the loop in continuous cycles of deign-implementation-evaluation. In this way, the development can focus on the most relevant issues for teachers or learners at all times.

          The evaluation of information visualization systems is essential. A plethora of techniques can be used, including controlled experiments that evaluate different visualization and interaction techniques or field studies that assess the impact of a visualization on learning (Plaisant, 2004). The latter take place in natural environments (classrooms) but are often time consuming and difficult to replicate and generalize (Nagel et al., 2014). Verbert et al. (2014) suggest the following evaluation techniques: 

1.  Effectiveness, which can refer to engagement, higher grades or post-test results, higher retention rates, improved self-assessment, and overall course satisfaction.
2.  Efficiency in the use of time of a teacher or learner.
3.  Usability and usefulness evaluations often focus on teachers being able to identify learners at risk or asking learners how well they think they are performing in a course.

          Typical evaluation instruments include questionnaires or controlled experiments where time-to-task, errors made, time-to-learn, et cetera are evaluated (Dillenbourg et al., 2011).
</div>
<div className="content-chunk" data-subsection-id = "Conclusion-475t">
## Conclusion
Information visualization concepts and methodologies are key enablers for

* Learners to gain insight into their learning actions and the effects these have.
* Teachers to stay aware of the subtle interactions in their courses.
* Researchers to discover patterns in large data sets of user traces and to communicate these data to their peers.

          As shown in this chapter, visualization has the unique potential to help shape the learning process and encourage reflection on its progress and impact by creating learning analytics dashboards that give a concise overview of relevant metrics in an actionable way and that support the exploration of patterns. 

          Designing and creating an effective information visualization system or learning analytics is an art, as the designer needs both domain expertise on learning theories and paradigms as well as techniques ranging from visual design to algorithm design (Nagel, 2015; Spence, 2001). In this chapter, we have briefly introduced the various steps in a visualization design process, from raw data analysis to effective dashboards evaluated by target users.
</div>
<div className="content-chunk" data-subsection-id = "References-97pt">
<Info title="REFERENCES">  
1 Arnold, K. E., & Pistilli, M. D. (2012). Course Signals at Purdue: Using learning analytics to increase student success. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge (LAK ’12), 29 April-2 May 2012, Vancouver, BC, Canada (pp. 267-270). New York: ACM.  
2 Bakharia, A., & Dawson, S. (2011). SNAPP: A bird's-eye view of temporal participant interaction. Proceedings of the 1st International Conference on Learning Analytics and Knowledge (LAK'11), 27 February-1 March 2011, Banff, AB, Canada (pp. 168-173). New York: ACM.  
3 Brusilovsky, P. (2000). Adaptive hypermedia: From intelligent tutoring systems to web-based education. In G. Gauthier, C. Frasson, K. VanLehn (Eds.), Proceedings of the 5th International Conference on Intelligent Tutoring Systems (ITS 2000), 19-23 June 2000, Montreal, QC, Canada (pp. 1-7). Springer. doi:10.1007/3-540-45108-0_1  
4 Card, S. K., Mackinlay, J., & Schneiderman, B. (Eds.). (1999). Readings in information visualization: Using vision to think. Burlington, MA: Morgan Kaufmann Publishers.  
5 Charleer, S., Klerkx, J., Ordiozola, S., Luis, J., & Duval, E. (2013). Improving awareness and reflection through collaborative, interactive visualizations of badges. In M. Kracik, B. Krogstie, A. Moore, V. Pammer, L. Panese, M. Prilla, W. Reinhardt, & T. D. Ullmann (Eds.), Proceedings of the 3rd Workshop on Awareness and Reflection in Technology Enhanced Learning (AR-TEL '13), 17 September 2013, Paphos, Cyprus (pp. pp. 69-81).  
6 Dillenbourg, P., Zufferey, G., Alavi, H., Jermann, P., Do-lenh, S., Bonnard, Q., Cuendet, S., & Kaplan, F. (2011). Classroom orchestration: The third circle of usability. Proceedings of the 9th International Conference on Computer-Supported Collaborative Learning (CSCL 2011), 4-8 July 2011, Hong Kong, China (vol. I, pp. 510-517). International Society of the Learning Sciences.  
7 Elliot, K. (2016). 39 studies about human perception in 30 minutes. Presented at OepnVis 2016. https://medium.com/@kennelliot/39-studies-about-human-perception-in-30-minutes-4728f9e31a73#.rs7z3ch6i  
8 Engelbart, D. (1995). Toward augmenting the human intellect and boosting our collective IQ. Communications of the ACM, 38(8), 30-34.  
9 Few, S. (2009). Now you see it: Simple visualization techniques for quantitative analysis. Berkeley, CA: Analytics Press.  
10 Govaerts, S., Verbert, K., Duval, E, & Pardo, A. (2012). The student activity meter for awareness and self reflection. CHI Conference on Human Factors in Computing Systems: Extended Abstracts (CHI EA '12), 5-10 May 2012, Austin, TX, USA (pp. 869-884). New York: ACM.  
11 Harrison, L., Yang, F., Franconeri, S., & Chang, R. (2014). Ranking visualizations of correlation using Weber's law. IEEE Transactions on Visualization and Computer Graphics, 20(12), 1943-1952.  
12 Heer, J., & Schneiderman, B. (2012, February). Interactive dynamics for visual analysis. Queue – Microprocessors, 10(2), 30.  
13 Levy, E., Zacks, J., Tversky, B., & Schiano, D. (1996, April). Gratuitous graphics? Putting preferences in perspective. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '96), 13-18 April 1996, Vancouver, BC, Canada (pp. 42-49). New York: ACM.  
14 Mackinlay, J. (1986). Automating the design of graphical presentations of relational information. Transactions on Graphics, 5(2), 110-141.  
15 McDonnel, B., & Elmqvist, N. (2009). Towards utilizing GPUs in information visualization: A model and implementation of image-space operations. IEEE Transactions on Visualization and Computer Graphics, 15(6), 1105-1112.  
16 Nagel;, T., Maitan, M., Duval, E., Vande Moere, A., Lerkx, J., Kloeckl, K., & Ratti, C. (2014). Touching transport: A case study on visualizing metropolitan public transit on interactive tabletops. Proceedings of the 12th International Working Conference on Advanced Visual Interfaces (AVI 2014), 27-29 May 2014, Como, Italy (pp. 281-288). New York: ACM.  
17 Nagel, T. (2015). Unfolding data: Software and design approaches to support casual exploration of tempo-spatial data on interactive tabletops. Leuven, Belgium: KU Leuven, Faculty of Engineering Science.  
18 Ochoa, X., Verbert, K. Chiluiza, K., & Duval, E. (2016). Uncertainty visualization in learning analytics. Work in progress for IEEE Transactions on Learning Technologies.  
19 Plaisant, C. (2004). The challenge of information visualization evaluation. Proceedings of the 2nd International Working Conference on Advanced Visual Interfaces (AVI '04), 25-28 May 2004, Gallipoli, Italy (pp. 109-116). New York: ACM  
20 Santos, O. C., Boticario, J. G., Romero, C., Pechenizkiy, M., Merceron, A., Mitros, P., Luna, J. M., Mihaescu, C., Moreno, P., Hershkovitz, A., Ventura, S., & Desmarais, M. (Eds.). (2015). Proceedings of the 8th International Conference on Educational Data Mining (EDM2015), 26-29 June 2015, Madrid, Spain. International Educational Data Mining Society.  
21 Shneiderman, B., & Bederson, B. B. (2003). The craft of information visualization: Readings and reflections. Burlington, MA: Morgan Kaufmann Publishers.  
22 Spence, R. (2001). Information visualization: Design for interaction, 1st ed. Salt Lake City, UT: Addison-Wesley.  
23 Tufte, E. R. (2001). The visual display of quantitative information, 2nd ed. Cheshire, CT: Graphics Press LLC.  
24 Verbert, K., Govaerts, S., Duval, E., Santos, J., Assche, F., Parra, G., & Klerkx, J. (2014). Learning dashboards: An overview and future research opportunities. Personal and Ubiquitous Computing, 18(6), 1499-1514.  
25 Ware, C. (2004). Information visualization: Perception for design, 2nd ed. Burlington, MA: Morgan Kaufmann Publishers.  
</Info>
</div>
